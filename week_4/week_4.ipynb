{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining models\n",
    "\n",
    "Within this API we have `tf.keras.Model` class for models. `keras` models are basic computational units that transform input $x$ to output $\\hat{y}$ and that can be trained via SGD or similar algorithms. \n",
    "\n",
    "We will define it using predefined `Layer`s. Compared to `keras` models, layers are more atomic computational units, that can be reused, e.g. `Dense` layer is an implementation of MLP layer equation: $\\sigma(\\mathbf{Wx} + \\mathbf{b})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(keras.Model):  # Subclassing\n",
    "    \n",
    "    def __init__(self, dim_output, dim_hidden):\n",
    "        super(MultilayerPerceptron, self).__init__(name='multilayer_perceptron')\n",
    "        self.dim_output = dim_output\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "        # Within Model.__init__ we initialize all the layers we will use\n",
    "        self.layer_1 = keras.layers.Dense(\n",
    "            units=dim_hidden)  # units = how many neurons in the layer\n",
    "        self.layer_2 = keras.layers.Dense(\n",
    "            units=dim_output)\n",
    "\n",
    "    def call(self, x):  # call defines the flow of the computation, e.g. in this particular model\n",
    "                        # we simply call the two layers one after the oter\n",
    "        h = self.layer_1(x)\n",
    "        y = self.layer_2(h)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models\n",
    "\n",
    "We will train this model to classify the _Iris_ dataset from previous lab. Training models defined like this is really easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(  # We create a new model\n",
    "    dim_output=3,\n",
    "    dim_hidden=32)\n",
    "\n",
    "model.compile(  # By compiling we prepare the model for training\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003),  # We pick a optimizer algorithm\n",
    "    loss='mean_squared_error',  # We pick a loss function\n",
    "    metrics=['accuracy'])  # We pick evaluation metrics\n",
    "\n",
    "model.fit(  # Fit runs the training over provided data\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the selling point for using modern neural frameworks. The model is trained via SGD, but we do not need to calculate derivatives. Instead they are calculated automatically by TF. We also do not need to program how SGD works, nor we need to define the loss functions or metrics.\n",
    "\n",
    "All that we done manually last week is now hidden behind the `fit` function. You should already be familiar with all the concepts that were introduced in the code above, such as `epochs`, `batch_size`, `metrics`, `loss`, `optimizer`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32,\n",
    "    num_layers=3,\n",
    "    activation=keras.activations.sigmoid)\n",
    "\n",
    "# compile and fit are the same as above\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tape\n",
    "\n",
    "`fit` is a very convenient way of training neural models, but sometimes we need more flexibility and control. For example, with `fit` we can not track the training step by step (e.g. for debugging). The model is compiled into a computation graph in the background. So if you want to have a debugging print within a model, it will not run. E.g., try printing the value of `h` in the model `call`.\n",
    "\n",
    "Instead we can use so called `GradientType`. With this tape the debugging print of `h` will run. Check the following code, it is very similar in how we defined SGD in previous labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "\n",
    "# loss_function = keras.losses.CategoricalCrossentropy()\n",
    "# You can use cross-entropy loss if you completed PA 4.3\n",
    "    \n",
    "def step(xs, ys):  # This has the same meaning as step function in previous labs\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(xs)  # Model predictions\n",
    "        loss = loss_function(ys, preds)  # The value of loss function comparing the true\n",
    "                                         # values ys with predictions\n",
    "\n",
    "    gradient = tape.gradient(\n",
    "        target=loss,\n",
    "        sources=model.trainable_variables)  # Calculate the gradient of loss function w.r.t. model parameters.\n",
    "                                            # This behaves the same as gradient methods from previous labs.\n",
    "        \n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))  # Applies the computed gradient on current\n",
    "                                                                         # parameter values.\n",
    "    \n",
    "def loss(xs, ys):\n",
    "    preds = model(xs)\n",
    "    return loss_function(ys, preds)\n",
    "    \n",
    "num_epochs = 100\n",
    "batch_size = 5\n",
    "num_samples = len(data.x)\n",
    "\n",
    "# Training loop (without shuffling for simplicity)\n",
    "for e in range(num_epochs):\n",
    "    for i in np.arange(0, num_samples, batch_size):  # Batching\n",
    "        step(data.x[i:i+batch_size], data.y[i:i+batch_size])\n",
    "    print('Epoch:', e, 'Loss:', loss(data.x, data.y).numpy())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
