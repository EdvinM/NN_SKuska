{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "## Key Concepts from This Week\n",
    "\n",
    "- Multilayer perceptron\n",
    "- Layer (input, hidden, output)\n",
    "- Activation function\n",
    "- Classification\n",
    "- Random initialization\n",
    "- Train and test set\n",
    "---\n",
    "\n",
    "## Multilayer Perceptron\n",
    "\n",
    "- This classical model is still widely used for processing vector data. \n",
    "- It can also be called  _feed-forward_ model or _dense_ model in literature. The iconic illustration of neural networks shows this model:\n",
    "\n",
    "![Multilayer Perceptron](images/mlp.svg)\n",
    "<center><small>License: Glosser.ca <a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>, via Wikimedia Commons</small></center>\n",
    "\n",
    "- Models like these consistsof several layers. \n",
    "- First, there is an input layer - with model above we have an input with _three_ features. Then a series of neuron layers $\\mathbf{h}_1, ..., \\mathbf{h}_K$ comes. \n",
    "- Each layer consists of several artificial neurons, where $i$-th neuron of $j$-th layer is defined as:\n",
    "\\begin{equation}\n",
    "h_j^{i} = \\sigma_j(\\mathbf{w_j^i} \\cdot \\mathbf{h_{j-1}} + b_j^i)\n",
    "\\end{equation}\n",
    "- $\\mathbf{w}_j^i$ and $b_j^i$ are parameters (weights and bias) for this particular neuron. \n",
    "- $\\mathbf{h}_{j-1}$ is the vector of values calculated for the neurons from previous layer (with $h_0$ being the input layer) and $\\sigma$ is the activation function for this particular layer. \n",
    "- Note that each neuron \"sees\" _all_ the neurons from the previous layer.\n",
    "\n",
    "\n",
    "### Output\n",
    "- The final layer $h_K$ serves as an output of the model $\\mathbf{\\hat{y}}$. \n",
    "- Note that we can have multiple output neurons, i.e. we can predict vectors of values, not only scalars. All the layers between input and ouput layers are considered _hidden_ layers.\n",
    "\n",
    "- The prediction $\\mathbf{\\hat{y}}$ from output layer is compared with expected value $\\mathbf{y}$ via loss function, in the very same way we used these two term in previous lab, e.g. by using _mean squared error_. \n",
    "- This loss function can be minimized by _stochastic gradient descent_ algorithm. \n",
    "- The SGD for training neural networks is identical with the SGD algorithm we used to train linear regression.\n",
    "\n",
    "\n",
    "### Defining MLP with Vectors and Matrices\n",
    "\n",
    "Simplify the calculations with matrix operations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z}_j = \\mathbf{W}_j \\mathbf{h}_{j-1} + \\mathbf{b}_j\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_j = \\sigma_j(z_j)\n",
    "\\end{equation}\n",
    "\n",
    "- Mathematically, this is the same as calculating the neuron one by one. \n",
    "- Note that $i$-th row of $\\mathbf{W}_j$ and $i$-th member of $\\mathbf{b}_j$ are in fact the parameters for $i$-th neuron:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}_j = \\begin{bmatrix}\\mathbf{w}_j^1 \\\\ \\mathbf{w}_j^2 \\\\ \\vdots \\\\ \\mathbf{w}_j^M \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "- We introduced the $\\mathbf{z}$ quantity because it will come handy later. \n",
    "- It is sometimes called neuron pre-activation value.\n",
    "\n",
    "\n",
    "### Architecture Decisions\n",
    "\n",
    "__1. How many layers should the network have?__  \n",
    "For smaller datasets you can easily go with only one or two hidden layers. For bigger dataset or more difficult tasks the number of layers grows and they can use tens of layers. Huge state-of-the art image recognition systems have [~100 convolutional hidden layers]\n",
    "\n",
    "__2. How many neurons should be in the layers?__  \n",
    "You can start with ~100 neurons for smaller datasets. The biggest current models can use several thousands of neurons per layer. The number of neurons is usually the same for all the hidden layers.\n",
    "\n",
    "For both number of layers and number of neurons you should check relevant literature to see how big the models are for comparable datasets. We will talk about how to correctly set parameters like these in the future. \n",
    "\n",
    "__3. What activation functions should be used for individual layers?__  \n",
    "All the hidden layers usually use the same activation function. You can use either _ReLU_ or some of its variants, such as _Leaky ReLU_ as a good starting point.\n",
    "\n",
    "For output layer the activation function is usually different. Here we need to customize the function to fit the task, e.g. when we do regression and we want to have the results between 0 and 1, we can use the logistic regression function. When we do regression for all real numbers, we can use linear function instead.\n",
    "\n",
    "### Training MLP\n",
    "\n",
    "MLP can be trained with stochastic gradient descent. The general outline of the SGD algorithm is exactly the same as with linear regression from last week's lab. The equations for the derivatives are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{W}_n} = \\frac{dL}{d\\mathbf{z}_n} \\mathbf{h}_{n-1}^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{db_n} = \\frac{dL}{d\\mathbf{z}_n}\n",
    "\\end{equation}\n",
    "\n",
    "Next we need to calculate $\\frac{dL}{d\\mathbf{z}_n}$. For all layers, but the last it is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{z}_n} = (\\mathbf{W}_{n+1}^T\\frac{dL}{d\\mathbf{z}_{n+1}}) \\odot \\sigma_n'(\\mathbf{z}_n)\n",
    "\\end{equation}\n",
    "\n",
    "$\\odot$ is [Hadamard product]. Note that in the last term we do not use the activation function $\\sigma$, but its derivative $\\sigma'$, e.g. if the activation function would be $x^2$, its derivative used here would be $2x$.\n",
    "\n",
    "For the last $K$-th layer the equation is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{z}_K} = \\frac{dL}{d\\mathbf{h}_K} \\odot \\sigma_K'(\\mathbf{z}_K)\n",
    "\\end{equation}\n",
    "\n",
    "The term $\\frac{dL}{d\\mathbf{h}_K}$ is calculated according to the definition of loss function.\n",
    "\n",
    "## Classification\n",
    "\n",
    "With classification we aim to assign each sample to a class, while we have a predefined finite set of $C$ classes. E.g. we might want to take measurments of _Iris_ flowers and classify them into one of three possible _Iris_ species.\n",
    "\n",
    "The data look like this:\n",
    "\n",
    "| | | | | |\n",
    "| --- | --- | --- | --- | - |\n",
    "| 5.0 | 3.3 | 1.4 | 0.2 | 0 |\n",
    "| 7.0 | 3.2 | 4.7 | 1.4 | 1 |\n",
    "| 5.7 | 2.8 | 4.1 | 1.3 | 1 |\n",
    "| 6.3 | 3.3 | 6.0 | 2.5 | 2 |\n",
    "\n",
    "The first four columns are measurments of the flowers:\n",
    "\n",
    "- sepal length in cm\n",
    "- sepal width in cm\n",
    "- petal length in cm\n",
    "- petal width in cm\n",
    "\n",
    "The last column is a code fo _Iris_ species:\n",
    "0. _Iris Setosa_\n",
    "1. _Iris Versicolour_\n",
    "2. _Iris Virginica_\n",
    "\n",
    "<img src=\"images/iris.jpg\" alt=\"Iris\" width=\"500\"/>\n",
    "<center><small>License: Davefoc <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons</small></center>\n",
    "\n",
    "To solve a general classification problem we propose a simple MLP model with one hidden layer. The two layers are defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z}_1 = \\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h} = \\sigma_1(\\mathbf{z}_1)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z}_2 = \\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{y}} = \\sigma_2(\\mathbf{z}_2)\n",
    "\\end{equation}\n",
    "\n",
    "This just follows the general equation of MLP we showed above. The hidden layer $\\mathbf{h}$ has a customizable size that can be set before the training. The output layer will have a size of 3, one output neuron for each _Iris_ class.\n",
    "\n",
    "We will use logistic regression function for both $\\sigma_1$ and $\\sigma_2$. In the hidden layer, it simply serves as a non-linear function. In the output layer, it squishes the $\\mathbf{z}_2$ into $(0, 1)$ range and thus we can interpret the outputs $\\mathbf{\\hat{y}}$ as probabilities. The $j$-th element of $\\mathbf{\\hat{y}}$ tells us what is the probability that the sample belongs to the $j$-th class. E.g. with ${\\mathbf{\\hat{y}} = [0.1, 0.8, 0.3]$ we can say that the model gives 10% probability for the first class, 80% for the second class and 30% for the third class. Note that they do not add up to 100%, this model calculates the probability for each class independently. \n",
    "\n",
    "The definition of logistic function $\\sigma$ is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "We need to have a loss function that will compare the predicted probabilities with the true value - the true class of the sample. To do so we will apply _mean squared error_ loss function on the predicted values $\\mathbf{\\hat{y}}$ and the true value encoded with _one-hot encoding_. One-hot encoding of $j$-th class makes a vector of size $C$, where all components are $0$, except for the $j$-th component, which is $1$.\n",
    "\n",
    "_Example:_ $\\mathbf{\\hat{y}}$ is a prediction that the model calculates. $\\mathbf{y}$ is a true label we want to achieve. In this case the second element is $1$, so it encodes the second class -- _Iris Versicolour_.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{y}} = \\begin{bmatrix}0.1 \\\\ 0.8 \\\\ 0.3\\end{bmatrix}\\ \\ \\ \\mathbf{y} = \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The loss function for $i$-th sample is then defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L^{(i)} = \\frac{\\sum_{j=1}^C{(\\hat{y}_j^{(i)} - y_j^{(i)})^2}}{C}\n",
    "\\end{equation}\n",
    "\n",
    "The overall loss function is defined as an average of loss functions for all the samples, similarly as before.\n",
    "\n",
    "__Practical Note:__ Logistic function and MSE are not usually used as activation function and loss function for classification problems. We have better options, but we will use these two in this lab because of their simplicity. In practice, we would use _softmax_ activation and _cross-entropy_ loss function. More information about these two is in the _Further Reading_ section.\n",
    "\n",
    "### Training classifier\n",
    "\n",
    "We need to calculate the derivatives for SGD. Following the general equations from earlier we can define the derivatives of parameters $\\theta = \\{ \\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2 \\}$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{z}_2} = 2(\\mathbf{\\hat{y}} - \\mathbf{y}) \\odot \\sigma'(\\mathbf{z}_2)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{W}_2} = \\frac{dL}{d\\mathbf{z}_2} \\mathbf{h}^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{b}_2} = \\frac{dL}{d\\mathbf{z}_2}\n",
    "\\end{equation}\n",
    "\n",
    "These are the definitions for second layer parameters. The derivative of activation function is: $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$. Otherwise these equations should be easy to understand. Then for the first layer, we just follow the general equations from before:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{z}_1} = (\\mathbf{W}_2^T\\frac{dL}{d\\mathbf{z}_2}) \\odot \\sigma'(\\mathbf{z}_1)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{d\\mathbf{W}_1} = \\frac{dL}{d\\mathbf{z}_1} \\mathbf{x}^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dL}{db_1} = \\frac{dL}{d\\mathbf{z}_1}\n",
    "\\end{equation}\n",
    "\n",
    "For all these parameter matrices and vectors the SGD update rule is the same as before:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{i+1} = \\theta_i - \\alpha \\frac{dL}{d\\theta_i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating models\n",
    "\n",
    "Minimizing loss function is how we train neural networks. However it has two drawbacks for other practical uses:\n",
    "\n",
    "1. The loss function describes how well we fit the data we show the model. But we want to know how well does the model work on data it has not seen before. E.g. machine translation system that can only translate the sentences it has seen before is not very useful.\n",
    "\n",
    "To solve this issue we split the data we have into two __non-overlapping__ sets - _training_ set and _testing_ set. Training set is used to train the model, i.e. to directly minimize the loss function. Testing set is then used to evaluate how well does the model work on data it has _not seen_ before.\n",
    "  \n",
    "\n",
    "2. Loss function is a rather abstact quantity. It is hard to tell how well does a model with loss $0.2$ perform. or whether a model with loss $0.19$ is significantly better.\n",
    "\n",
    "To solve this issue, we should use a more straightforward metric for evaluation. For example for classification we can use an _accuracy_ instead. Accuracy tells us the percentage of samples we are able to classify successfully - i.e. how many samples has the highest prediction value for correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
