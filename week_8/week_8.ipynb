{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You need to run this cell for the code in following cells to work.\n",
    "\"\"\"\n",
    "\n",
    "# Enable module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Enable interactive plots\n",
    "%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8\n",
    "\n",
    "__Goals for this week__\n",
    "\n",
    "We will discuss recurrent neural networks in this lab. They are useful for processing sequences of data, such as sentences, time series, etc. We will also talk about word representations for natural language processing.\n",
    "\n",
    "__Feedback__\n",
    "\n",
    "This lab is a work in progress. If you notice a mistake, notify us or you can even make a pull request. Also please fill the [questionnaire](https://forms.gle/r27nBAvnMC7jbjJ58) after you finish this lab to give us feedback.\n",
    "\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "_Recurrent neural networks_ (RNN) are the last major neural architecture we will talk about during our labs. They are used to process data sequences. One-dimensional CNNs can also be used for sequence processing, however, RNNs should be better at modeling long-term dependencies between individual inputs. RNNs are also more versatile for sequence data, e.g. they can be used for tasks that expect a sequence as an output, or that expect a separate label for each input.\n",
    "\n",
    "_Recurrent cell_ lies at the heart of RNNs. Cell is the basic operation that is done as we process one step from a series of $N$ inputs $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N$. For step $i$ the cell looks like this:\n",
    "\n",
    "   y\n",
    "> cell >\n",
    "   x\n",
    "   \n",
    "- $\\mathbf{x}_i$ is $i$-th input\n",
    "- $\\mathbf{y}_i$ is $i$-th output\n",
    "- $\\mathbf{s}_i$ is the state of cell for $i$-th step\n",
    "   \n",
    "All of these quantities are vectors. As the figure above illustrates, at each step the cell depends on two inputs - the input for the step itself and the state of the cell from previous step. Because the cell \"sees\" the state from previous steps, the layer can process the current step while using the knowledge about all the previous steps.\n",
    "\n",
    "We can imagine the recurrent layer as a series o cell operations:\n",
    "\n",
    "   y       y\n",
    "> cell > cell >\n",
    "   x       x\n",
    "   s vyznacenim trasy\n",
    "   \n",
    "Note that when we follow the flow of computation leading to output $\\mathbf{y}_i$, we can see that it depends on all the previous inputs $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_i$. It also depends on the initial state $\\mathbf{s}_0$, which is usually a trainable parameter vector of the model.\n",
    "\n",
    "### Recurrent cell variants\n",
    "\n",
    "Multiple variants of recurrent cells exist, e.g. this is the definition of _Ellman cell_:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i = \\sigma(\\mathbf{W}_{in}\\mathbf{x}_i + \\mathbf{W}_{hid}\\mathbf{s}_{i-1} + \\mathbf{b}_{hid}) \\\\\n",
    "\\mathbf{y}_i = \\sigma(\\mathbf{W}_{out}\\mathbf{h}_i + \\mathbf{b}_{out})  \\\\\n",
    "\\mathbf{s}_i = \\mathbf{h}_i \\\\\n",
    "$$\n",
    "\n",
    "The definition of $\\mathbf{h}_i$ and $\\mathbf{y}_i$ is very similar to MLP, the only difference is that $\\mathbf{h}_i$ also depends on the state from previous step cell. In this case the state $\\mathbf{s}_i$ is simply the value of hidden layer $\\mathbf{h}_i$ within the cell. Note that the same parameters (weights and biases) are used for each step. The computation done by a cell is the same for each step, only the inputs of the cell ($\\mathbf{x}_i$ and $\\mathbf{s}_{i-1}$) differ.\n",
    "\n",
    "Simple cells like these does not perform very well. The information is being transformed by matrix multiplication each time step. This tends to dilute the information and the network \"forgets\" about what it has seen in the past. This limits the use of simple recurrent cells only for relatively short sequences. Simple cells are also quite unstable to train and suffer from so called _exploding/vanishing gradient_ [FIXME: reference] problem, which makes them unstable to train.\n",
    "\n",
    "Instead of these simple cells we usually use more complex cells that were developed to address the issues we mentiond. Most common of these cells are _LSTM_ and _GRU_ cells [FIXME: references]. Check the further reading section if you are interested in why they tend to work better than vanilla recurrent cells.\n",
    "\n",
    "### Training\n",
    "\n",
    "The operation used for RNN are very similar as the oprations from MLP. We use matrix multiplication, addition, activation functions and that is basically all there is to it. The training routine is therefore also quite similar to MLP. Again, we simply use _stochastic gradient descent_ to calculate the derivatives of the loss function w.r.t. each parameter.\n",
    "\n",
    "### Recurrent architectures\n",
    "\n",
    "There are multiple ways of using recurrent layers depending on the nature of the task we want to solve. In all the following example the recurrent layer is the same, we only work differently with the inputs and outputs of this layer to get it to do what we want.\n",
    "\n",
    "#### Many to one\n",
    "\n",
    "We feed the recurrent layer until we process the whole input. Then we use the result of this pass to get a label. We use this type of RNN to do:\n",
    "\n",
    "- Sequence classification - We want to assign a label to a sequence (i.e. text classification, event detection).\n",
    "- Single-hop prediction - We want to predict singular next value.\n",
    "\n",
    "For final computation we can either:\n",
    "\n",
    "- Discard all the outputs, but the last $\\mathbf{y}_N$. Then we use only this output.\n",
    "- Pool all the outputs using max- or in this case more often mean-pooling of all the output $\\mathbf{y}_i$.\n",
    "\n",
    "#### One to many\n",
    "\n",
    "We feed the recurrent layer with one value and we expect it to produce multiple values. We use this type for:\n",
    "\n",
    "- Generation tasks - We want to generate a series of values based on a prompt (e.g. image captioning, music generation).\n",
    "\n",
    "The other than first cells also expect some input $\\mathbf{x}_{i>1}$, otherwise they can not compute further. We can either use the same input as for the first cell $\\mathbf{x}_1$, or we can feed them the output from previous step $\\mathbf{y}_{i-1}$.\n",
    "\n",
    "#### Many to many\n",
    "\n",
    "We feed the recurrent layer with multiple values and we expect an output for each of them. We use this type for:\n",
    "\n",
    "- Input tagging - We want to assign each input into a class (e.g. part-of-speech tagging, event scope detection).\n",
    "\n",
    "#### Sequence to sequence\n",
    "\n",
    "We feed the recurrent layer a series of inputs and we expect a series as an output. We use thys type for:\n",
    "\n",
    "- Multi-hop prediction - We want to generate multiple values as a prediction.\n",
    "\n",
    "### Advanced recurrent architectures\n",
    "\n",
    "The architecture mentioned above show how can a single recurrent layer be used. In this section we show some example of how to combine multiple layers for various use-cases.\n",
    "\n",
    "#### Bi-directional recurrent network\n",
    "\n",
    "We can combine two recurrent layers, one that processes the data from start to end, while the other goes backwards from end to start. We simply combine the outputs of these two networks for each time step. The advantage of this combination is that for each time step the following layer \"sees\" all the inputs, not only the previous ones. This is depicted in the figure by red outline\n",
    "\n",
    "#### Multilayer recurrent network\n",
    "\n",
    "We can also simply stack multiple recurrent layers on top of each other. This is mainly used to increase the capacity of the model. Usually RNNs are not as deep as CNNs and we use up to 5 layers. One layer is usually enough as a starting point.\n",
    "\n",
    "#### Hierarchical recurrent network\n",
    "\n",
    "For sequences of sequences (e.g. sentence is a sequence of words and words are sequences of characters) hierarchical recurrent networks can be used. We again combine two networks. In this case, the first processes the words character by character. The outputs of this network for each word are then fed to another RNN.\n",
    "\n",
    "#### Encoder-decoder architecture\n",
    "\n",
    "We can combine two recurrent layers for sequence to sequence tasks as well. We then have one layer that encodes the input into a representation and the other that decodes this representation into a series of outputs. The main use case for this architecture is machine translation.\n",
    "\n",
    "### Time series prediction example\n",
    "\n",
    "Show task\n",
    "\n",
    "Exercise which of the previously mentioned RNN architecture would you use?\n",
    "\n",
    "\n",
    "\n",
    "## Architecture comparison\n",
    "\n",
    "We have discussed three major neural architectures during our labs. Each is well suited for different kind of data:\n",
    "\n",
    "1. _Multilayer Perceptron._ Used for fixed size feature vectors.\n",
    "2. _Convolutional Neural Networks_. Used for fixed size 1D, 2D or 3D data with strong spatial relation.\n",
    "3. _Recurrent Neural Networks._ Used for sequences.\n",
    "\n",
    "We can combine these architectures, e.g. we have seen convolutional neural networks with dense layers at the top. We can also combine convolutional and recurrent networks for video processing. (We process each video frame with convolutional layers. Then we take these frame representations and feed them into a recurrent network.)\n",
    "\n",
    "Some of these combinations are considered specific architectures on their own, e.g.:\n",
    "\n",
    "- _Autoencoder:_ Used for learning compact representations.\n",
    "- _Generational Adversarial Networks:_ Used to generate images and sounds.\n",
    "- _Graph Neural Networks:_ Used to process graphs.\n",
    "\n",
    "There are also many additional architecture, that are different from the three we mentioned. These can include recursive networks, self-organizing maps, attention-based networks (e.g. Transformer architecture) and many others. These are mostly used only occasionally.\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "RNNs are often used for _natural language processing_ (NLP). Words are sequences of letters, sentences are sequences of words, documents are sequences of paragraphs. The written and spoken language are both very sequential in nature. Another fine feature of RNNs is that they are quite versatile. Their different forms, as depicted above, can be used for structurally different tasks:\n",
    "\n",
    "- many-to-one - text classification\n",
    "- many-to-many - part-of-speech tagging\n",
    "- sequence-to-sequence - machine translation\n",
    "- one to many - image labeling\n",
    "\n",
    "But, how should we feed text into neural models? The most common way is to feed the networks word by word, while each word is represented by its `id` - an integer identifying each particular word form:\n",
    "\n",
    "```\n",
    "load_data\n",
    "print_data\n",
    "print word2id fict\n",
    "print ids\n",
    "```\n",
    "\n",
    "Then we can use so called _embedding_ layer to get a vector representation from each `id`. Embedding layer has a vector representation assigned for each id an\n",
    "\n",
    "```\n",
    "keras example\n",
    "```\n",
    "\n",
    "Note, that this is logically the same thing as having each word represented by one-hot representation and then multiplying this with W. Embedding layer is used because simply selecting n-th row is more efficient than multiplying magrices.\n",
    "\n",
    "\n",
    "Data loaded this way have batch_size, time_size, emb_dim shape and they can be used for further calculations, e.g. a simple text classification model:\n",
    "\n",
    "```\n",
    "emb\n",
    "lstm\n",
    "dense\n",
    "```\n",
    "\n",
    "### Pre-trained embeddings\n",
    "\n",
    "The embeddings are essentially representations of words, i.e. they should encode semantic information about the meaning of the words. Because many NLP tasks need the same information, we can actually take the embeddings trained for one task and reuse them for other task. This practically means that we can simply take the matrix W and use it later.\n",
    "\n",
    "Large portion of parameters\n",
    "Rare words\n",
    "Time efficiency\n",
    "\n",
    "There are multiple libraries used to generate word embeddings. Some of the earliest were word2vec and GloVe. However I would recommend using fastText, they should perform better. Often you can simply download pre-trained embedding matrices from the internet, e.g. list of fastText embeddings for various languages.\n",
    "\n",
    "Then we can load the vectors as our embedding matrix initializer:\n",
    "\n",
    "\n",
    "train or do not train embedding layer?\n",
    "\n",
    "make sure your vocabularies match\n",
    "\n",
    "__Exercise 8.X:__ TensorBoard can also visualize word embeddings so we can explore them. Exercise: play with tensorboard, try to find what words are familiar\n",
    "\n",
    "__Exercise 8.X:__ Maybe word2vec online to see what are the most similar words?\n",
    "\n",
    "### PA: 8.X LSTM-based POS tagger [2 pts]\n",
    "\n",
    "Part-of-speech tagging (_POS tagging_) is a classical NLP task. We want to mark each word in a sentence with a correct POS tag. POS tags are grammatical categories of words, such as _verb_, _noun_, etc. The data for this task consist of sentences and their respective POS tags:\n",
    "\n",
    "POS data example\n",
    "\n",
    "We are essentially trying to perform a classification for each word.\n",
    "\n",
    "We can solve this task with RNN architecture depicted below:\n",
    "\n",
    "softmax softmax softmax\n",
    "\n",
    "dense   dense    dense\n",
    "\n",
    "bi-dir lstm\n",
    "\n",
    "wordem  wordem   wordem\n",
    "\n",
    "Implement this architecture in keras. There are several gotchas:\n",
    "\n",
    "masking\n",
    "bidir\n",
    "dense over multiple time steps\n",
    "loss is meanpooled\n",
    "\n",
    "### Other forms\n",
    "\n",
    "char, subword based representation\n",
    "LM pre-training\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "C.Olah LSTM\n",
    "BPTT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from week_8.data.sin import sin_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 29s 29ms/sample - loss: 0.1372\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 27s 27ms/sample - loss: 0.0670\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 27s 27ms/sample - loss: 0.0401\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 29s 29ms/sample - loss: 0.0244\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 35s 35ms/sample - loss: 0.0169\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 32s 32ms/sample - loss: 0.0123\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 30s 30ms/sample - loss: 0.0091\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 29s 29ms/sample - loss: 0.0071\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 29s 29ms/sample - loss: 0.0048\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 29s 29ms/sample - loss: 0.0031\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 28s 28ms/sample - loss: 0.0031\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 28s 28ms/sample - loss: 0.0030\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 27s 27ms/sample - loss: 0.0026\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 33s 33ms/sample - loss: 0.0016\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 32s 32ms/sample - loss: 0.0016\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 33s 33ms/sample - loss: 0.0020\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 27s 27ms/sample - loss: 0.0031\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 32s 32ms/sample - loss: 0.0010\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 36s 36ms/sample - loss: 7.9644e-04\n",
      "Epoch 20/50\n",
      " 390/1000 [==========>...................] - ETA: 18s - loss: 6.8690e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-815e8d2cc6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     )\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "import tensorflow as tf\n",
    "\n",
    "class LSTModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTModel, self).__init__()\n",
    "        self.lstm = tf.keras.layers.LSTM(20)\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.lstm(x)  # <- expects (batch_size, time_steps, sample_dim) shape\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "model = LSTModel()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse')\n",
    "\n",
    "x, y = sin_dataset(1000, 900, 10)\n",
    "model.fit(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    epochs=50,\n",
    "    batch_size=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
