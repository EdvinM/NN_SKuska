{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "- _Recurrent neural networks_ (RNN) are the last major neural architecture we will talk about during our labs. - They are used to process sequence data. \n",
    "- One-dimensional CNNs can also be used for sequence processing, however, RNNs should be better at modeling long-term dependencies between individual inputs. \n",
    "- RNNs are also more versatile for sequence data, e.g. they can be used for tasks that expect a sequence as an output, or that expect a separate label for each input.\n",
    "\n",
    "- _Recurrent cell_ lies at the heart of RNNs. Cell is the basic operation that is done as we process one step from a series of $N$ inputs $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N$. For step $i$ the cell looks like this:\n",
    "\n",
    "<img src=\"images/cell.svg\" alt=\"Recurrent cell\" style=\"width: 20%;\"/>\n",
    "   \n",
    "- $\\mathbf{x}_i$ is $i$-th input\n",
    "- $\\mathbf{y}_i$ is $i$-th output\n",
    "- $\\mathbf{s}_i$ is the state of cell for $i$-th step\n",
    "   \n",
    "- All of these quantities are vectors. \n",
    "- As the figure above illustrates, at each step the cell depends on two inputs - the input for the step itself and the state of the cell from previous step. \n",
    "- Because the cell \"sees\" the state from previous steps, the layer can process the current step while using the knowledge about all the previous steps.\n",
    "\n",
    "We can imagine the recurrent layer as a series of cell operations:\n",
    "\n",
    "<img src=\"images/rnn.svg\" alt=\"Recurrent layer\" style=\"width: 50%;\"/>\n",
    "   \n",
    "- Note that when we follow the flow of computation leading to output $\\mathbf{y}_i$, we can see that it depends on all the previous inputs $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_i$. \n",
    "- It also depends on the initial state $\\mathbf{s}_0$, which is usually a trainable parameter vector of the model.\n",
    "\n",
    "### Recurrent cell variants\n",
    "\n",
    "Multiple variants of recurrent cells exist, e.g. this is the definition of _Ellman cell:_\n",
    "\n",
    "$\n",
    "\\mathbf{h}_i = \\sigma(\\mathbf{W}_{in}\\mathbf{x}_i + \\mathbf{W}_{hid}\\mathbf{s}_{i-1} + \\mathbf{b}_{hid}) \\\\\n",
    "\\mathbf{y}_i = \\sigma(\\mathbf{W}_{out}\\mathbf{h}_i + \\mathbf{b}_{out})  \\\\\n",
    "\\mathbf{s}_i = \\mathbf{h}_i \\\\\n",
    "$\n",
    "\n",
    "The definition of $\\mathbf{h}_i$ and $\\mathbf{y}_i$ is very similar to MLP, the only difference is that $\\mathbf{h}_i$ also depends on the state from previous step cell. In this case the state $\\mathbf{s}_i$ is simply the value of hidden layer $\\mathbf{h}_i$ within the cell. Note that the same parameters (weights and biases) are used for each step. The computation done by a cell is the same for each step, only the inputs of the cell ($\\mathbf{x}_i$ and $\\mathbf{s}_{i-1}$) differ.\n",
    "\n",
    "Simple cells like these do not perform very well. The information is being transformed by matrix multiplication each time step. This tends to dilute the information and the network \"forgets\" about what it has seen in the past. This limits the use of simple recurrent cells only for relatively short sequences. Simple cells are also quite unstable to train and suffer from so called _exploding / vanishing gradient_ problem.\n",
    "\n",
    "Instead of these simple cells we usually use more complex cells that were developed to address the issues we mentioned. Most common of these cells are _LSTM_ and _GRU_ cells. Check the further reading section if you are interested in why they tend to work better than vanilla recurrent cells or how do they look like.\n",
    "\n",
    "### Training\n",
    "\n",
    "The operations used for RNN are very similar to MLP operations. We use matrix multiplication, addition, activation functions and that is basically all there is to it. The training routine is therefore also quite similar to MLP. Again, so in previous architectures, we use _stochastic gradient descent_ to calculate the derivatives of the loss function w.r.t. each parameter.\n",
    "\n",
    "### Recurrent architectures\n",
    "\n",
    "There are multiple ways of using recurrent layers depending on the nature of the task we want to solve. In all the following examples the recurrent layer is the same, we only work differently with the inputs and outputs of this layer to get it to do what we want.\n",
    "\n",
    "#### Many to one\n",
    "\n",
    "<img src=\"images/manyone.svg\" alt=\"Many to one\" style=\"width: 40%;\"/>\n",
    "\n",
    "We feed the recurrent layer until we process the whole input. Then we use the result of this pass to get a single result. We use this type of RNN to do:\n",
    "\n",
    "- Sequence classification - We want to assign a label to a sequence (i.e. text classification, event detection).\n",
    "- Prediction - We want to predict following values in a time series.\n",
    "\n",
    "During the computation we can either:\n",
    "\n",
    "- Discard all the outputs, but the last $\\mathbf{y}_N$. Then we use only this output.\n",
    "- Pool all the outputs using mean-pooling (or max-pooling) of all the outputs $\\mathbf{y}_i$.\n",
    "\n",
    "#### One to many\n",
    "\n",
    "<img src=\"images/onemany.svg\" alt=\"One to many\" style=\"width: 40%;\"/>\n",
    "\n",
    "We feed the recurrent layer with one value and we expect it to produce multiple values. We use this type for:\n",
    "\n",
    "- Generation tasks - We want to generate a series of values based on a prompt (e.g. image captioning, music generation).\n",
    "\n",
    "During all the steps the cell expects an input $\\mathbf{x}_{i>1}$, that is how it is defined. We can either use the same input each step, or we can feed the layer with the output from previous step $\\mathbf{y}_{i-1}$.\n",
    "\n",
    "#### Many to many\n",
    "\n",
    "<img src=\"images/manymany.svg\" alt=\"Many to many\" style=\"width: 40%;\"/>\n",
    "\n",
    "We feed the recurrent layer with multiple values and we expect an output for each of them. We use this type for:\n",
    "\n",
    "- Input tagging - We want to assign each input into a class (e.g. part-of-speech tagging, event scope detection).\n",
    "\n",
    "#### Sequence to sequence\n",
    "\n",
    "<img src=\"images/seqseq.svg\" alt=\"Sequence to sequence\" style=\"width: 70%;\"/>\n",
    "\n",
    "We feed the recurrent layer a series of inputs and we expect a series as an output. We use thys type for:\n",
    "\n",
    "- Multi-hop prediction - We want to generate multiple values as a prediction.\n",
    "\n",
    "### Advanced recurrent architectures\n",
    "\n",
    "The architecture mentioned above show how can a single recurrent layer be used. In this section we show some examples of how to combine multiple layers for various use-cases.\n",
    "\n",
    "#### Bi-directional recurrent network\n",
    "\n",
    "<img src=\"images/bidirectional.svg\" alt=\"Bi-directional\" style=\"width: 70%;\"/>\n",
    "\n",
    "We can combine two recurrent layers, one that processes the data from start to end, while the other goes backwards from end to start. We simply combine the outputs of these two networks for each time step. The advantage of this combination is that for each time step the following layer \"sees\" all the inputs, not only the previous ones. Follow the flow of computation going into any output $\\mathbf{y}_i$. You will find out that it connects to all the inputs.\n",
    "\n",
    "#### Multilayer recurrent network\n",
    "\n",
    "<img src=\"images/multilayer.svg\" alt=\"Multilayer\" style=\"width: 60%;\"/>\n",
    "\n",
    "We can also simply stack multiple recurrent layers on top of each other. This is mainly used to increase the capacity of the model, i.e. its ability to model data. Usually RNNs are not as deep as CNNs and we use up to 5 layers. One layer is usually enough as a starting point.\n",
    "\n",
    "#### Hierarchical recurrent network\n",
    "\n",
    "<img src=\"images/hierarchical.svg\" alt=\"Hierarchical\" style=\"width: 70%;\"/>\n",
    "\n",
    "For sequences of sequences (e.g. sentence is a sequence of words and words are sequences of characters) hierarchical recurrent networks can be used. We again combine two networks. In the sentence-word-character case, the first processes the words character by character. The outputs of this network for each word are then fed to another RNN.\n",
    "\n",
    "#### Encoder-decoder architecture\n",
    "\n",
    "<img src=\"images/encoderdecoder.svg\" alt=\"Encoder-decoder\" style=\"width: 80%;\"/>\n",
    "\n",
    "We can combine two recurrent layers for sequence to sequence tasks as well. We then have one layer that encodes the input into a representation and the other that decodes this representation into a series of outputs. The main use case for this architecture is machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs for Natural Language Processing\n",
    "\n",
    "RNNs are often used for _natural language processing_ (NLP). Words are sequences of letters, sentences are sequences of words, documents are sequences of paragraphs. Both written and spoken language are sequential in nature. RNNs are appropriate choice for sequence processing. In addition, RNNs are also quite versatile, as we have seen before. Their different architectures can be used for different NLP tasks.\n",
    "\n",
    "But, how should we feed text into neural models? The most common way is to feed the text word by word. Each word is represented by its `id` - a unique integer identifier. We get this `id` simply by constructing a vocabulary of all the words we have in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence is then simply a sequence of integers. We feed this sequence into neural networks via a so called _embedding layer_. For a vocabulary of size $V$, such layer contains an _embedding matrix_ $\\mathbf{E}$ with $V$ rows. In this matrix, $i$-th row is a vector representation for word with `id == i`. When we have a vocabulary with 500 word, $\\mathbf{E}$ will have 500 rows and in each row a vector representation for one word will be stored. The number of columns - how long are the word representations - is a hyperparameter. It is usually in the order of hundreds.\n",
    "\n",
    "Below is how embedding layer for vocabulary with $V = 5$ and embedding size $3$ works on a sequence of integers $\\mathbf{s}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{E} = \\begin{vmatrix}\n",
    "e_{11} & e_{12} & e_{13} \\\\\n",
    "e_{21} & e_{22} & e_{23} \\\\\n",
    "e_{31} & e_{32} & e_{33} \\\\\n",
    "e_{41} & e_{42} & e_{43} \\\\\n",
    "e_{51} & e_{52} & e_{53}\n",
    "\\end{vmatrix}\n",
    "\\quad\n",
    "\\mathbf{s} = \\langle 1, 3, 5, 1 \\rangle\n",
    "\\quad\n",
    "emb_{\\mathbf{E}}(\\mathbf{s}) = \\begin{vmatrix}\n",
    "e_{11} & e_{12} & e_{13} \\\\\n",
    "e_{31} & e_{32} & e_{33} \\\\\n",
    "e_{51} & e_{52} & e_{53} \\\\\n",
    "e_{11} & e_{12} & e_{13} \\\\\n",
    "\\end{vmatrix} \\\\\n",
    "$$\n",
    "\n",
    "The embedding layer is implemented in `keras`. The output from an embedding layer can then be directly fed into a recurrent layer. The code below shows an example of a simple LSTM-based text classificator:\n",
    "\n",
    "```python\n",
    "class TextClassificator(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_units. num_classes):   \n",
    "        super(TextClassificator, self).__init__()\n",
    "        \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size)\n",
    "        \n",
    "        self.lstm =LSTM(\n",
    "            units=lstm_units)\n",
    "        \n",
    "        self.dense = Dense(\n",
    "            units=num_classes,\n",
    "            activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Pre-trained embeddings\n",
    "\n",
    "Each row of $\\mathbf{E}$ is a vector representationf for one word. It encodes the semantic information about the word. Because many NLP tasks need the same information, we can actually reuse the trained $\\mathbf{E}$ in other models. This is a type of _transfer learning_. If we store the trained $\\mathbf{E}$, we can then later simply initialize the embedding with this matrix in other models:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "matrix = np.array([...])  # 2D numpy array\n",
    "\n",
    "self.emb = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    embeddings_initializer=Constant(matrix))\n",
    "```\n",
    "\n",
    "When we reuse pre-trained embeddings, we need to make sure that the vocabularies match, i.e. that the ids of the words are the same. If the model that trained the embeddings had the word _dog_ with `id == 1`, the model that uses the emebddings should also have `id == 1` for _dog_.\n",
    "\n",
    "This technique of reusing embeddings is especially useful when we do not have much training data. The pre-trained embeddings provide information that can bootstrap the model. E.g. normally, if the model does not encounter the word _cat_ during the training, it does not know what to do with this word during the evaluation. However, with pre-trained embeddings, the model already saw similar words, such as _dog_. The model then can assume that the words should behave similarly.\n",
    "\n",
    "Often we fix the values of pre-trained embeddings during the training. That means that the value of $\\mathbf{E}$ is not updated by the training algorithm and it will stay the same throughout the whole training. Whether to train the pre-train embedding matrix is also a hyperparameter. It is usually not recommended to train the pre-trained embeddings, unless you have a relatively big dataset. You can control this behavior by using `trainable` parameter of `Embedding` layer:\n",
    "\n",
    "```python\n",
    "self.emb = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    embeddings_initializer=Constant(matrix),\n",
    "    trainable=False)\n",
    "```\n",
    "\n",
    "#### Embedding libraries\n",
    "\n",
    "There are multiple libraries used to generate word embeddings that you can use: `word2vec`, `GloVe`, `fastText`, and many others. They train the word representation on language modeling tasks. I would recommend using `fastText`, empirically it has a very good performance.\n",
    "\n",
    "You can pre-train your own embeddings with these libraries by providing them a text corpus. Or even better, you can simply download the pre-trained representations that someone else already created, e.g. `fastText` has [published their representations for many languages](https://fasttext.cc/docs/en/crawl-vectors.html). The pre-trained embeddings are simply a file where in each row we have a word and its vector written. Check out the `/week_8/data/embeddings` file in the repository.\n",
    "\n",
    "#### Vocabularies in practice\n",
    "\n",
    "We operate with two vocabularies, a vocabulary of words in our dataset and a vocabulary of words for which we have pre-trained embeddings. These two usually do not overlap perfectly. We have two issues:\n",
    "\n",
    "1. _Useless pre-trained embeddings._ We can have pre-trained representations for words that are not present in the training or testing data. We can simply discard embeddings like these during an experiment.\n",
    "2. _Missing pre-trained embeddings._ We can have words in our data for which we do not have pre-trained embeddings. There are multiple strategies of handling words like these:\n",
    "\n",
    "  - First, we should check for pre-trained embeddings for similar words. Maybe we need to lowecase the word or add/remove diacritics.\n",
    "  - We can create a random vector for words like these.\n",
    "  - We can discard such words and use a special `<unknown>` token instead of them. This token can simply have a zero vector as an embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "The data come from [Slovenský Národný Korpus (Slovak National Corpus) dataset](https://github.com/UniversalDependencies/UD_Slovak-SNK). We used only a limited amount of 200 sentences for both training and testing set. The pre-trained embeddings come from the official [fastText repository](). The original embeddings have 2M words in them, here we provide only the embeddings for the words you need (~2000 words)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
