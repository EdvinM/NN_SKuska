{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra\n",
    "\n",
    "Neural network models can be defined using vectors and matrices, i.e. concepts from linear algebra. You should know how basic linear operations work. Some of the concepts were covered during your _Algebra and Discrete Mathematics_ course. Read the provided links to review neccessary topics (note that there are some questions at the end of each page) and solve the exercises in this notebook.\n",
    "\n",
    "#### Vectors\n",
    "- [On vectors](https://www.mathsisfun.com/algebra/vectors.html)\n",
    "- [On dot product](https://www.mathsisfun.com/algebra/vectors-dot-product.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "- Almost all of neural network training done in practice is currently based on methods that calculate the derivatives with respect to (w.r.t.) parameters of the model.\n",
    "- Partial derivatives are applied when we derive a function with more than one variable. In such case we can actually derive a function in any direction.\n",
    "\n",
    "### Gradient\n",
    "- Vector of derivatives w.r.t. all the parameters is called a _gradient_. \n",
    "Generaly for function $f$ with arbitrary number of parameters $x_1. x_2, ..., x_N = \\mathbf{x}$, the gradient $\\triangledown f$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\triangledown f(\\mathbf{x}) = \\frac{df}{d\\mathbf{x}} = \\begin{bmatrix}\\frac{df}{dx_1} \\\\ \\frac{df}{dx_2} \\\\ \\vdots \\\\ \\frac{df}{dx_N} \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Gradient is the most important concept from this week's lab. The gradient is a vector quantity that tells us the _direction of steepest ascent_ at each point. This is a very important property, which we will often use in the following weeks. The magnitude of this vector tells us how steep this ascent is, i.e. what is the slope of the tangent in the direction of the gradient.\n",
    "\n",
    "To cpmpare _derivative_ and _gradient_:\n",
    "\n",
    "- _Derivative_ is a quantity that tells us, what is the rate of change in given direction.\n",
    "- _Gradient_ is a quantity that tells us what is the direction of the steepest rate of change, along with the rate of this change.\n",
    "\n",
    "Observe the difference between these two concepts in the Figure below. All the plots show the same function $F(x,y) = \\sin(x) \\cos(y)$. In first two plots we shot the derivatives w.r.t $y$ and $x$ respectively. These are shown as white arrows. Notice that they all point in one direction. On the other hand in the last plot we show the gradients. If we interpret the derivatives from the two previous plots as vectors, these gradients are in fact their sum.\n",
    "\n",
    "![Derivatives and gradient](images/derivatives.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
