{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "- In previous labs we introduced various quantities that control how does the model train or how does the model look like, e.g. _learning rate,_ _batch size,_ _hidden layer size_ and others. \n",
    "- These quantities are called _hyperparameters_. \n",
    "- In contrast to regular parameters, they are set before the training takes place and they are not modified during the gradient descent training. \n",
    "- The values we choose for the hyperparameters can significantly change the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Hyperparameter tuning,_ a process of searching for better hyperparameter values, should be done during each machine learning project. \n",
    "- We expect you to tune your hyperparameters during your course projects as well. \n",
    "- The goal of hyperparameter tuning is to find a set of hyperparameters that optimizes some evaluation metric, e.g. for classification we want to find hyperparametes that has the best accuracy of the model. \n",
    "- We do this by searching through space of all possibel values either via manual tuning or via automatic tuning.\n",
    "\n",
    "### Manual tuning\n",
    "\n",
    "- Manual tuning is a name for a trial-and-error tuning that is done by hand. \n",
    "- You essentialy look at how did the model train and try to guess which hyperparameter needs to change. \n",
    "- Then you train a model with new hyperparameters and see how did it affect the results. \n",
    "- You repeat this until you are content with the performance of your model.\n",
    "\n",
    "This process is of course quite unsystematic and ineffective. Even for professionals it is hard to guess which hyperparameters are problematic. With this technique you also often explore only a small subspace of all possible values and perhaps you might miss global optimum entirely.\n",
    "\n",
    "### Random search tuning\n",
    "\n",
    "- Random search is a more systematic approach. \n",
    "- You set a interval of possible values for each hyperparameter. \n",
    "- Then you randomly sample from these intervals and train the model with what you sampled. \n",
    "- You repeat this process until you are content with the performance, or until you have computational resources.\n",
    "\n",
    "This approach demands more compute than the manual tuning, but it can be fully automatized. Getting better performance is a function of time in this case, you are bound to find better and better models as the time goes on. How you set the intervals for each hyperparameter and how you sample from this interval is very important. Setting these wrong can make the process inefficient.\n",
    " \n",
    "### Hyperparameter properties\n",
    "\n",
    "Hyperparameters can be divided by their type:\n",
    "\n",
    "- _Integer hyperparameters,_ e.g. hidden layer size, number of hidden layers\n",
    "- _Real number hyperparameters,_ e.g. learning rate\n",
    "- _Categorical hyperparameter,_ e.g. activation function\n",
    "\n",
    "For number hyperparameters (both integer and real) we sample from a pre-defined range during random search. E.g. for hidden layer size we might define the minimum value as 10 and the maximum value as 1000. We then pick the value from within this range. There are two basic ways of sampling number from within this range:\n",
    "\n",
    "- _Linear,_ when we simply pick random value from within this range using uniform distribution.\n",
    "- _Exponential,_ when we define the range via exponents as $\\langle 10^1, 10^3\\rangle$. Instead of sampling from 10 to 1000, we sample from 1 to 3 interval. This skews the distribution towards the smaller numbers, e.g. in this case half of the values will fall into $\\langle 10, 100 \\rangle$ interval, while the other half will fall into $\\langle 100, 1000 \\rangle$ interval, even though the second interval is in fact 10 times bigger.\n",
    "\n",
    "Below we list some hyperparameters you already encountered with recommended starting ranges.\n",
    "\n",
    "__Learning rate__ - real - exponential - $\\langle 10^{-2}, 10^{-4} \\rangle$.  \n",
    "- Learning rate is the most important hyperparameter that should always be tuned. \n",
    "- Setting it too low will halt the training as it can get stuck in plateaus or it can pointlessly make the training longer. \n",
    "- Setting it too high might cause divergence that can lead to numeric overflow exception.\n",
    "\n",
    "__Batch size__ - integer - exponential - $\\langle 2^3, 2^6 \\rangle$.  \n",
    "- Using larger batch size provides faster training as we can use parallelism abilities of modern HW. \n",
    "- However the model performance can decrease. \n",
    "- Some more complex tasks (e.g. game playing bot) are trained with extremely big batch size in the order of millions.\n",
    "\n",
    "__Hidden layer size__ - integer - exponential - $\\langle 2^5, 2^8 \\rangle$.  \n",
    "- Setting the hidden layer too small can low model capacity. \n",
    "- Capacity is the ability of ML models to model the data. \n",
    "- E.g. linear regression has very small capacity, because it can only model linear relations. \n",
    "- Setting the hidden layer too big can cause overfitting. We will talk about overfitting in following labs.\n",
    "\n",
    "__Number of layers__ - integer - linear - $\\langle 1, 5 \\rangle$.  \n",
    "- Compared to previous hyperparameters, number of layers if often architecture specific. \n",
    "- For MLP model we learned about so far we usually work with relatively small number of layers. \n",
    "- More layers are often used in computer vision convolutional neural networks.\n",
    "\n",
    "__Activation function__ - categorical - { relu, sigmoid, ... }.  \n",
    "- Activation function can be experimented with, but is usually not so important. \n",
    "- ReLU is usually a good starting point. For really small models you can use sigmoid instead.\n",
    "\n",
    "__Loss function__ - categorical - { cross-entropy + softmax activation, MSE + linear activation, ... }  \n",
    "- There are some loss functions that are usually used for some tasks, e.g. you should use cross-entropy with softmax for classification or MSE for regression. \n",
    "\n",
    "There are multiple aspects we need to take into consideration when we choose the hyperparameters, e.g. batch size, layer size and number of layers also influence how big is the model memory-wise. Setting these parameters too high can make the it too big for training on available hardware. We are usually limited by the size of RAM in our HW accelerators (e.g. GPU cards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning in practice\n",
    "\n",
    "1. You usually start with manual tuning during the development, when you just want to quickly see whether the model works and is able to learn. As soon as you have your model ready and you want to properly train and evaluate it, you should switch to random tuning.\n",
    "\n",
    "2. Check the hyperparameter values people use in recent (2014 or later) and related (same dataset or task) projects. They should server as a fine starting point.\n",
    "\n",
    "3. You can gradually change the search intervals. E.g. if you find out that a certain subspace has good results, you can focus on this subspace. Similarly, you can expand the range of some parameter, if the best results are achieved with its marginal values. E.g. with batch sizes 4, 8 and 16 you always have the best results with 16. Then it makes sense to expand the batch size range to 32 and perhaps 64 as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
